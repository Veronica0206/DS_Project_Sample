### Company XYZ is an Online Travel Agent site, such as Expedia, Booking.com, etc. They haven't invested in data science yet and all the data they have about user searches are simply stored in the URLs generated when users search for a hotel. If you are not familiar with URLs, you can run a search on any OTA site and see how all search parameters are present in the URL.

### You are asked to:

#### (1) Create a clean data set where each column is a field in the URL, each row is a given search and the cells are the corresponding URL values.

#### (2) For each search query, how many amenities were selected?

#### (3) Often, to measure the quality of a search algorithm, data scientists use some metric based on how often users click on the second page, third page, and so on. The idea here is that a great search algorithm should return all interesting results on the first page and never force users to visit the other pages (how often do you click on the second page results when you search on Google? Almost never, right?). Create a metric based on the above idea and find the city with the worst search algorithm.

Require needed packages and source codes
----------------------------------------

Read in dataset
---------------

Create a clean data set with all possible fields as columns
-----------------------------------------------------------

### Step 1: Parse URLs

### Step 2: Split parameter: Based on the code book, the number of fields could be up to 23 (including the subfields of hotel.amenities), we then just split the parameters to 23 columns and then address them further.

    get_fields <- parsed_url %>% 
      separate(parameter, paste0("V", 1:23), sep = "&")

    ## Warning: Expected 23 pieces. Missing pieces filled with `NA` in 77677
    ## rows [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
    ## 20, ...].

    fields_wide <- data.frame(search_id = 1:nrow(get_fields), get_fields[, 5:27])

### Step 3: Convert the wide format data to long format data and then separate the values from search key words

### Step 4: Create an expand grids with all possible combinations of search\_id and fields, then left join the result of Step 3.

    fields <- c("hotel.checkin", "hotel.customMinimumPriceFilter", 
                "hotel.customMaximumPriceFilter", "hotel.freeCancellation",
                "hotel.stars_5", "hotel.stars_4", "hotel.stars_3", "hotel.stars_2",
                "hotel.stars_1", "hotel.max_score", "hotel.min_score", 
                "hotel.couponCode", "hotel.adults", "hotel.city",
                "hotel.children", "shuttle", "internet", "breakfast", "lounge",
                "yes_smoking", "yes_pet", "hotel.checkout", "hotel.search_page")
    amenities.index <- which(fields_long$fields == "hotel.amenities")
    amenities <- data.frame(fields_long[amenities.index, ], check = "yes")
    names(amenities)[3:5] <- c("amenities", "fields", "values")
    re_fields_long <- rbind(fields_long[-amenities.index, ], amenities[, -3])
    all.possible_long <- as.data.frame(expand.grid(search_id = 1:nrow(fields_wide),
                                                   fields = fields)) %>% 
      left_join(re_fields_long, by = c("search_id", "fields")) %>% arrange(search_id)

    ## Warning: Column `fields` joining factor and character vector, coercing into
    ## character vector

### Convert the long format with all possible fields to the wide format

Answer questions
----------------

### (1) Create a clean data set where each column is a field in the URL, each row is a given search and the cells are the corresponding URL values.

Through above steps, we've already get required data set, and using
`head()` function, we may view the first three observations.

    ##   search_id hotel.checkin hotel.customMinimumPriceFilter
    ## 1         1    2015-09-19                           <NA>
    ## 2         2    2015-09-14                           <NA>
    ## 3         3    2015-09-26                           <NA>
    ##   hotel.customMaximumPriceFilter hotel.freeCancellation hotel.stars_5
    ## 1                           <NA>                   <NA>          <NA>
    ## 2                           <NA>                   <NA>          <NA>
    ## 3                            175                   <NA>          <NA>
    ##   hotel.stars_4 hotel.stars_3 hotel.stars_2 hotel.stars_1 hotel.max_score
    ## 1           yes          <NA>          <NA>          <NA>            <NA>
    ## 2          <NA>           yes          <NA>          <NA>            <NA>
    ## 3           yes          <NA>          <NA>          <NA>            <NA>
    ##   hotel.min_score hotel.couponCode hotel.adults
    ## 1               4             <NA>            3
    ## 2               4             <NA>            3
    ## 3               5             <NA>            2
    ##                    hotel.city hotel.children shuttle internet breakfast
    ## 1 New York, NY, United States           <NA>    <NA>     <NA>      <NA>
    ## 2      London, United Kingdom           <NA>    <NA>     <NA>      <NA>
    ## 3 New York, NY, United States           <NA>    <NA>     <NA>      <NA>
    ##   lounge yes_smoking yes_pet hotel.checkout hotel.search_page
    ## 1   <NA>        <NA>    <NA>     2015-09-20                 1
    ## 2   <NA>        <NA>    <NA>     2015-09-15                 1
    ## 3   <NA>        <NA>    <NA>     2015-09-27                 1

### (2) For each search query, how many amenities were selected?

    ## 
    ##     0     1     2 
    ## 76973   699     5

    ## [1] 334 418 533 539 699 700

    ## [1] 28367 38354 66116 66117 66118

    ##     shuttle    internet   breakfast      lounge yes_smoking     yes_pet 
    ##         111         272          40          22         174          90

    ## character(0)

    ## [1] "yes_smoking"

    ## [1] "yes_smoking" "yes_pet"

### (3) Create a metric based on the above idea and find the city with the worst search algorithm.

Based on the idea described in the challenge, I would like to create a
flag with 1 for only 1 search page and 0 otherwise. And we then define
the metric as the failure rate of finding results on the first page.

    ## # A tibble: 4 x 5
    ##   hotel.city                    total_no success_no failure_no failure_rate
    ##   <chr>                            <int>      <int>      <int>        <dbl>
    ## 1 Hong Kong, Hong Kong             11786      10735       1051       0.0892
    ## 2 London, United Kingdom           28058      14775      13283       0.473 
    ## 3 New York, NY, United States      29384      16385      12999       0.442 
    ## 4 San Francisco, California, Uâ€¦     8449       8105        344       0.0407

Based on the defined metric, the city with the worst search algorithm
was London, United Kingdom, and then New York, NY, United States. The
algorithm for the other cities worked well.
